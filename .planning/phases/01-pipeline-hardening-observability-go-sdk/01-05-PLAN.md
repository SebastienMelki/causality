---
phase: 01-pipeline-hardening-observability-go-sdk
plan: 05
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - internal/dlq/module.go
  - internal/dlq/internal/service/dlq_service.go
  - internal/nats/stream.go
  - internal/nats/config.go
autonomous: true

must_haves:
  truths:
    - "CAUSALITY_DLQ NATS stream exists for failed messages"
    - "Events exceeding MaxDeliver are captured in DLQ stream"
    - "DLQ depth is tracked via DLQDepth metric"
    - "DLQ events are queryable via NATS subject"
  artifacts:
    - path: "internal/dlq/module.go"
      provides: "DLQ module facade"
      exports: ["Module", "New"]
    - path: "internal/dlq/internal/service/dlq_service.go"
      provides: "DLQ consumer and advisory listener"
      contains: "advisorySubject"
    - path: "internal/nats/stream.go"
      provides: "Updated stream manager with DLQ stream creation"
      contains: "CAUSALITY_DLQ"
  key_links:
    - from: "internal/dlq/internal/service/dlq_service.go"
      to: "NATS advisory subject"
      via: "subscribes to $JS.EVENT.ADVISORY.CONSUMER.MAX_DELIVERIES"
      pattern: "MAX_DELIVERIES"
    - from: "internal/nats/stream.go"
      to: "jetstream.StreamConfig"
      via: "creates CAUSALITY_DLQ stream"
      pattern: "CAUSALITY_DLQ"
---

<objective>
Create the dead letter queue module and extend the NATS stream manager to support a DLQ stream. Failed events (after max retries) are captured for investigation.

Purpose: R1.7 requires events that fail processing after N retries to be moved to a DLQ. This prevents poison messages from blocking consumers and provides a debugging surface for failed events.
Output: `internal/dlq/` module with advisory listener and DLQ consumer. Updated NATS stream manager with DLQ stream creation.
</objective>

<execution_context>
@/Users/sebastienmelki/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sebastienmelki/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-pipeline-hardening-observability-go-sdk/01-RESEARCH.md
@.planning/phases/01-pipeline-hardening-observability-go-sdk/01-01-SUMMARY.md
@internal/nats/stream.go
@internal/nats/config.go
@internal/nats/client.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend NATS stream manager with DLQ stream and update consumer MaxDeliver</name>
  <files>
    internal/nats/stream.go
    internal/nats/config.go
  </files>
  <action>
    **Update `internal/nats/config.go`:**
    - Add `DLQStreamName string` to StreamConfig (env: `NATS_DLQ_STREAM_NAME`, default: `CAUSALITY_DLQ`)
    - Add `DLQMaxAge time.Duration` to StreamConfig (env: `NATS_DLQ_MAX_AGE`, default: `720h` — 30 days)
    - Ensure `MaxDeliver` in ConsumerConfig defaults to a reasonable value (e.g., 5) if not already set. Check `DefaultConsumerConfigs()` and set `MaxDeliver: 5` for both warehouse-sink and analysis-engine consumers.

    **Update `internal/nats/stream.go`:**
    - Add `EnsureDLQStream(ctx context.Context) (jetstream.Stream, error)` method to `StreamManager`:
      ```go
      func (m *StreamManager) EnsureDLQStream(ctx context.Context) (jetstream.Stream, error) {
          dlqCfg := jetstream.StreamConfig{
              Name:     m.config.DLQStreamName,
              Subjects: []string{"dlq.>"},
              Storage:  jetstream.FileStorage,
              MaxAge:   m.config.DLQMaxAge,
          }
          // Same create-or-update pattern as EnsureStream
      }
      ```
    - The DLQ stream captures messages republished to `dlq.>` subjects
    - Use the same create-or-update pattern as the existing `EnsureStream` method
  </action>
  <verify>
    - `go build ./internal/nats/...` compiles
    - `grep "CAUSALITY_DLQ" internal/nats/stream.go` shows DLQ stream
    - `grep "MaxDeliver" internal/nats/config.go` shows MaxDeliver config
  </verify>
  <done>
    NATS stream manager can create both the main event stream and the DLQ stream. Consumer configs have MaxDeliver set to limit retry count before advisory is emitted.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create DLQ module with advisory listener</name>
  <files>
    internal/dlq/module.go
    internal/dlq/internal/service/dlq_service.go
  </files>
  <action>
    **Create `internal/dlq/internal/service/dlq_service.go`:**
    - `DLQService` struct:
      - `js jetstream.JetStream`
      - `nc *nats.Conn` (raw NATS connection for advisory subscription)
      - `metrics *observability.Metrics` (optional)
      - `logger *slog.Logger`
      - `streamName string` (main stream name, e.g., "CAUSALITY_EVENTS")
      - `consumerNames []string` (consumer names to monitor, e.g., ["warehouse-sink", "analysis-engine"])
      - `sub []*nats.Subscription` (advisory subscriptions)
    - `NewDLQService(js jetstream.JetStream, nc *nats.Conn, streamName string, consumerNames []string, metrics *observability.Metrics, logger *slog.Logger) *DLQService`
    - `Start(ctx context.Context) error`:
      - For each consumer name, subscribe to advisory subject:
        `$JS.EVENT.ADVISORY.CONSUMER.MAX_DELIVERIES.<streamName>.<consumerName>`
      - Advisory handler:
        1. Parse advisory JSON to extract `stream_seq` (the sequence number of the failed message)
        2. Fetch the original message by sequence from the stream using `js.Stream().GetMsg()`
        3. Republish to DLQ stream: `js.Publish(ctx, "dlq."+rawMsg.Subject, rawMsg.Data)` with headers preserving original subject and metadata
        4. Increment `metrics.DLQDepth` counter
        5. Log at WARN level: "event moved to DLQ", with stream_seq, consumer, subject
      - On advisory parse failure: log error, continue (don't crash)
    - `Stop()`:
      - Unsubscribe from all advisory subscriptions
    - `GetDLQCount(ctx context.Context) (int64, error)`:
      - Get DLQ stream info, return message count (for monitoring/alerting)

    **Create `internal/dlq/module.go`:**
    - `Config` struct:
      - `AlertThreshold int64` (env: `DLQ_ALERT_THRESHOLD`, default: 100)
    - `Module` struct holding `*service.DLQService`
    - `New(js jetstream.JetStream, nc *nats.Conn, streamName string, consumerNames []string, cfg Config, metrics *observability.Metrics, logger *slog.Logger) *Module`
    - `Start(ctx context.Context) error` — delegates to service
    - `Stop()` — delegates to service
    - `GetDLQCount(ctx context.Context) (int64, error)` — delegates to service

    Note: The NATS raw connection (`*nats.Conn`) is needed for subscribing to advisory subjects, which are core NATS subjects, not JetStream subjects. The existing `nats.Client` struct likely has a method or field to access the raw connection — check `internal/nats/client.go` for `Conn()` or similar.
  </action>
  <verify>
    - `go build ./internal/dlq/...` compiles
    - `go vet ./internal/dlq/...` passes
    - `grep "MAX_DELIVERIES" internal/dlq/internal/service/dlq_service.go` confirms advisory pattern
    - `grep "dlq\\." internal/dlq/internal/service/dlq_service.go` confirms republish to DLQ subject
  </verify>
  <done>
    DLQ module listens for NATS MaxDeliver advisories, captures failed messages, republishes them to the DLQ stream, and tracks depth via metrics. Module follows hexagonal pattern.
  </done>
</task>

</tasks>

<verification>
- `go build ./internal/dlq/...` and `go build ./internal/nats/...` compile
- NATS stream manager has EnsureDLQStream method
- Consumer configs have MaxDeliver set
- DLQ service subscribes to MaxDeliver advisory
- Failed messages are republished to "dlq.>" subjects
- DLQ depth is tracked via metrics
</verification>

<success_criteria>
- CAUSALITY_DLQ stream created on startup with 30-day retention
- Messages exceeding MaxDeliver (5 retries) trigger advisory
- Advisory listener captures original message and republishes to DLQ
- DLQ depth metric incremented on each capture
- DLQ count queryable for monitoring
- Module follows hexagonal pattern
</success_criteria>

<output>
After completion, create `.planning/phases/01-pipeline-hardening-observability-go-sdk/01-05-SUMMARY.md`
</output>
