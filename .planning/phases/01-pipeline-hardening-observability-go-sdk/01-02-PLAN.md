---
phase: 01-pipeline-hardening-observability-go-sdk
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - internal/warehouse/consumer.go
  - internal/warehouse/config.go
  - cmd/warehouse-sink/main.go
  - docker-compose.yml
autonomous: true

must_haves:
  truths:
    - "NATS messages are ACKed only after successful S3 upload"
    - "NATS messages are NAKed on S3 upload failure for redelivery"
    - "Poison messages (unmarshal failure) are terminated, not retried"
    - "Multiple worker goroutines fetch and process messages in parallel"
    - "Graceful shutdown flushes all in-flight batches before exiting"
    - "Shutdown has a configurable timeout to prevent deadlock"
    - "Docker Compose stop_grace_period is set for warehouse-sink"
  artifacts:
    - path: "internal/warehouse/consumer.go"
      provides: "Refactored consumer with ACK-after-write and worker pool"
      contains: "trackedEvent"
    - path: "internal/warehouse/config.go"
      provides: "WorkerCount and ShutdownTimeout config fields"
      contains: "WorkerCount"
    - path: "cmd/warehouse-sink/main.go"
      provides: "Wired observability module + shutdown timeout"
      contains: "observability"
  key_links:
    - from: "internal/warehouse/consumer.go"
      to: "jetstream.Msg"
      via: "trackedEvent retains msg reference through batch-flush cycle"
      pattern: "trackedEvent"
    - from: "internal/warehouse/consumer.go"
      to: "msg.Ack()"
      via: "ACK called only after writePartition succeeds"
      pattern: "msg\\.Ack"
    - from: "cmd/warehouse-sink/main.go"
      to: "internal/observability"
      via: "imports and initializes observability module"
      pattern: "observability\\.New"
---

<objective>
Refactor the warehouse consumer to ACK messages only after successful S3 upload (ACK-after-write) and add a configurable worker pool for parallel message consumption. Fix graceful shutdown to flush all in-flight batches with timeout protection.

Purpose: This is the most critical reliability fix — the current code ACKs messages before S3 write, causing data loss on crash. This plan also adds parallel consumption for throughput.
Output: Refactored `internal/warehouse/consumer.go` with tracked events, worker pool, and proper shutdown. Updated `cmd/warehouse-sink/main.go` with observability wiring and shutdown timeout.
</objective>

<execution_context>
@/Users/sebastienmelki/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sebastienmelki/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-pipeline-hardening-observability-go-sdk/01-RESEARCH.md
@.planning/phases/01-pipeline-hardening-observability-go-sdk/01-01-SUMMARY.md
@internal/warehouse/consumer.go
@internal/warehouse/config.go
@cmd/warehouse-sink/main.go
@docker-compose.yml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Refactor warehouse consumer for ACK-after-write and worker pool</name>
  <files>
    internal/warehouse/consumer.go
    internal/warehouse/config.go
  </files>
  <action>
    **Config changes** (`internal/warehouse/config.go`):
    - Add `WorkerCount int` to batch config (env: `WORKER_COUNT`, default: 1)
    - Add `ShutdownTimeout time.Duration` to config (env: `SHUTDOWN_TIMEOUT`, default: 60s)
    - Add `FetchBatchSize int` to config (env: `FETCH_BATCH_SIZE`, default: 100)

    **Consumer refactor** (`internal/warehouse/consumer.go`):

    1. Define `trackedEvent` struct:
       ```go
       type trackedEvent struct {
           event *pb.EventEnvelope
           msg   jetstream.Msg
       }
       ```

    2. Change `batch` field from `[]*pb.EventEnvelope` to `[]trackedEvent`.

    3. Refactor `Start(ctx context.Context) error`:
       - Get stream and consumer as before
       - Launch `c.config.Batch.WorkerCount` goroutines using `sync.WaitGroup`
       - Each worker calls `c.workerLoop(ctx, consumer, workerID)`
       - Start flush timer goroutine as before
       - Wait for all workers + timer in a goroutine that closes `doneCh`

    4. Create `workerLoop(ctx context.Context, consumer jetstream.Consumer, id int)`:
       - Loop with select on `ctx.Done()`, `c.stopCh`, and default
       - In default: call `consumer.Fetch(c.config.Batch.FetchBatchSize, jetstream.FetchMaxWait(5*time.Second))`
       - On fetch error: log if not DeadlineExceeded, `time.Sleep(time.Second)` for backoff, continue
       - For each message in `msgs.Messages()`: call `c.processMessage(ctx, msg)`
       - Do NOT ACK/NAK in the worker loop — that happens in flush

    5. Refactor `processMessage(ctx context.Context, msg jetstream.Msg) error`:
       - Unmarshal proto from `msg.Data()`
       - On unmarshal failure: call `msg.Term()` (poison message, don't retry), log error, return nil (don't propagate error since we terminated the message)
       - On success: lock mutex, append `trackedEvent{event: &event, msg: msg}` to batch, check if should flush
       - If should flush: call `c.flush(ctx)`
       - Return nil on success (ACK happens in flush, not here)

    6. Refactor `flush(ctx context.Context) error`:
       - Lock mutex, swap batch as before but with `[]trackedEvent`
       - Group tracked events by partition key
       - For each partition: call `c.writePartition(ctx, key, partitionTracked)`
       - If ANY partition write fails: NAK ALL messages in that partition (`t.msg.Nak()`) so they are redelivered. Log error. Continue with other partitions.
       - If partition write succeeds: ACK all messages in that partition (`t.msg.Ack()`). Log any ACK errors.
       - Record metrics: `S3FilesWritten`, `NATSBatchSize`, `NATSFlushLatency`

    7. Update `groupByPartition` to work with `[]trackedEvent` and return `map[partitionKey][]trackedEvent`.

    8. Update `writePartition` to accept `[]trackedEvent` — extract events from tracked for Parquet conversion.

    9. Refactor `Stop(ctx context.Context) error`:
       - Close `stopCh` to signal workers
       - Create a shutdown context with `c.config.ShutdownTimeout`
       - Wait for `doneCh` (workers done) with timeout
       - Call `c.flush(shutdownCtx)` for final flush
       - If shutdown context expires, log warning about data loss risk
       - Return any error

    **Important:** Remove the ACK/NAK from the old message processing loop (lines 108-118 in current consumer.go). Messages are only ACKed/NAKed during flush.
  </action>
  <verify>
    - `go build ./internal/warehouse/...` compiles
    - `go vet ./internal/warehouse/...` passes
    - Verify no `msg.Ack()` calls outside of `flush()` method
    - Verify `msg.Term()` is called for unmarshal failures in `processMessage`
  </verify>
  <done>
    Consumer tracks NATS message references through the batch-flush cycle. ACK only happens after successful S3 upload. NAK on failure triggers redelivery. Poison messages are terminated. Worker pool enables parallel consumption.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire observability and shutdown timeout into warehouse-sink main</name>
  <files>
    cmd/warehouse-sink/main.go
    docker-compose.yml
  </files>
  <action>
    **Update `cmd/warehouse-sink/main.go`:**
    - Import `internal/observability`
    - After logger setup, create observability module: `obs, err := observability.New("warehouse-sink")`
    - Defer `obs.Shutdown(ctx)`
    - Create metrics: `metrics, err := observability.NewMetrics(obs.Meter())`
    - Pass metrics to consumer (add metrics parameter to `NewConsumer` if needed, or store on module)
    - Start a small HTTP server on a configurable port (default: `:9090`, env: `METRICS_ADDR`) that serves only `/metrics` using `obs.MetricsHandler()` and `/health` returning 200. This is separate from the main event ingestion server. Use `http.NewServeMux()` with just these two endpoints.
    - On shutdown: use `context.WithTimeout(context.Background(), cfg.Warehouse.ShutdownTimeout)` for the consumer Stop call instead of `context.Background()`
    - Add `MetricsAddr` to the Config struct (env: `METRICS_ADDR`, default: `:9090`)

    **Update `docker-compose.yml`:**
    - Add `stop_grace_period: 2m` to the `warehouse-sink` service (2x the 30s flush interval + buffer)
    - Add `METRICS_ADDR: ":9090"` environment variable
    - Add `SHUTDOWN_TIMEOUT: "90s"` environment variable
    - Expose port 9090 for metrics scraping (add to ports list)
  </action>
  <verify>
    - `go build ./cmd/warehouse-sink/...` compiles
    - `docker-compose.yml` is valid YAML (can parse without errors)
    - `grep "stop_grace_period" docker-compose.yml` shows the value for warehouse-sink
    - `grep "METRICS_ADDR" docker-compose.yml` shows the environment variable
  </verify>
  <done>
    Warehouse sink has observability wired in, metrics endpoint exposed, and graceful shutdown with configurable timeout. Docker Compose has stop_grace_period set appropriately.
  </done>
</task>

</tasks>

<verification>
- `go build ./cmd/warehouse-sink/...` compiles
- `grep "trackedEvent" internal/warehouse/consumer.go` shows the tracked event struct
- `grep "msg.Term()" internal/warehouse/consumer.go` shows poison message handling
- `grep "msg.Ack()" internal/warehouse/consumer.go` — ACK only appears in flush(), not in processMessage
- `grep "WorkerCount" internal/warehouse/config.go` shows worker pool config
- `grep "stop_grace_period" docker-compose.yml` shows 2m for warehouse-sink
</verification>

<success_criteria>
- Events are ACKed to NATS only after successful S3 upload
- Failed uploads result in NAK for redelivery
- Poison messages (unmarshal failures) are terminated, not retried indefinitely
- Worker pool is configurable and defaults to 1
- Shutdown flushes with timeout protection
- Docker Compose has appropriate stop_grace_period
- Prometheus metrics endpoint available on warehouse-sink
</success_criteria>

<output>
After completion, create `.planning/phases/01-pipeline-hardening-observability-go-sdk/01-02-SUMMARY.md`
</output>
