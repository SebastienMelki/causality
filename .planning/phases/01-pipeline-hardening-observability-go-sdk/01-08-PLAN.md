---
phase: 01-pipeline-hardening-observability-go-sdk
plan: 08
type: execute
wave: 3
depends_on: ["01-01", "01-02", "01-05"]
files_modified:
  - cmd/reaction-engine/main.go
  - internal/reaction/consumer.go
  - docker-compose.yml
autonomous: true

must_haves:
  truths:
    - "Reaction engine has Prometheus metrics at /metrics"
    - "Reaction engine has worker pool for message consumption"
    - "RulesEvaluated, AlertsFired, WebhookSuccess, WebhookFailure metrics recorded"
    - "DLQ module wired into reaction engine for failed message capture"
    - "Graceful shutdown with configurable timeout"
  artifacts:
    - path: "cmd/reaction-engine/main.go"
      provides: "Wired observability, DLQ, metrics endpoint, shutdown timeout"
      contains: "observability.New"
    - path: "internal/reaction/consumer.go"
      provides: "Worker pool + metrics instrumentation"
      contains: "WorkerCount"
  key_links:
    - from: "cmd/reaction-engine/main.go"
      to: "internal/observability"
      via: "creates observability module and metrics"
      pattern: "observability\\.New"
    - from: "cmd/reaction-engine/main.go"
      to: "internal/dlq"
      via: "creates DLQ module and starts advisory listener"
      pattern: "dlq\\.New"
---

<objective>
Wire observability and DLQ into the reaction engine, add worker pool to the reaction consumer, and add metrics instrumentation for rule evaluation, alerting, and webhook delivery.

Purpose: The reaction engine needs the same reliability and observability treatment as the warehouse sink. R1.5 requires metrics on all services, R1.6 requires consumer scaling, and R1.7 requires DLQ support.
Output: Updated reaction engine with observability, DLQ, worker pool, and metrics endpoint.
</objective>

<execution_context>
@/Users/sebastienmelki/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sebastienmelki/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-pipeline-hardening-observability-go-sdk/01-RESEARCH.md
@.planning/phases/01-pipeline-hardening-observability-go-sdk/01-01-SUMMARY.md
@.planning/phases/01-pipeline-hardening-observability-go-sdk/01-02-SUMMARY.md
@.planning/phases/01-pipeline-hardening-observability-go-sdk/01-05-SUMMARY.md
@cmd/reaction-engine/main.go
@internal/reaction/consumer.go
@internal/reaction/engine.go
@internal/reaction/dispatcher.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add worker pool and metrics instrumentation to reaction consumer</name>
  <files>
    internal/reaction/consumer.go
    internal/reaction/config.go
  </files>
  <action>
    **Update `internal/reaction/config.go`:**
    - Add `WorkerCount int` (env: `CONSUMER_WORKER_COUNT`, default: 1) to consumer-related config
    - Add `FetchBatchSize int` (env: `CONSUMER_FETCH_BATCH_SIZE`, default: 100)
    - Add `ShutdownTimeout time.Duration` (env: `SHUTDOWN_TIMEOUT`, default: 30s)

    **Refactor `internal/reaction/consumer.go`:**
    - Add `metrics *observability.Metrics` field to Consumer struct (can be nil for backward compat)
    - Add `workerCount int` and `fetchBatchSize int` fields
    - Update `NewConsumer` signature to accept metrics and worker config
    - Refactor `Start()` to launch multiple worker goroutines (same pattern as warehouse consumer in 01-02):
      - Use `sync.WaitGroup` for worker lifecycle
      - Each worker runs `workerLoop(ctx, consumer, workerID)`
      - Workers fetch and process messages independently
    - In `processMessage`:
      - On unmarshal failure: call `msg.Term()` (poison message), do NOT NAK
      - On success: process through engine and anomaly detector
      - Increment `metrics.NATSMessagesProcessed` for each message
      - Record `metrics.RulesEvaluated` when engine processes event
      - Record `metrics.AlertsFired` when engine triggers alert (this may need a return value from engine.ProcessEvent, or increment inside engine — use the simplest approach)
      - ACK on success, NAK on processing error (keep existing behavior but add metrics)
    - Add backoff on fetch errors (time.Sleep(time.Second)) like warehouse consumer
    - In `Stop()`: add shutdown timeout via context.WithTimeout

    **Note on metrics inside engine/anomaly/dispatcher:**
    The engine, anomaly detector, and dispatcher have their own internal logic. For Phase 1, add metrics at the consumer level (messages processed, messages errored). Deeper instrumentation inside engine.go, anomaly.go, and dispatcher.go can happen as part of test coverage plan or separately — don't refactor those files in this plan to keep scope bounded.
  </action>
  <verify>
    - `go build ./internal/reaction/...` compiles
    - `grep "WorkerCount\|workerLoop" internal/reaction/consumer.go` shows worker pool
    - `grep "msg.Term()" internal/reaction/consumer.go` shows poison message handling
    - `grep "metrics" internal/reaction/consumer.go` shows metrics instrumentation
  </verify>
  <done>
    Reaction consumer has configurable worker pool, poison message handling via Term(), metrics instrumentation, and backoff on fetch errors. Same reliability pattern as warehouse consumer.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire observability, DLQ, and metrics endpoint into reaction engine main</name>
  <files>
    cmd/reaction-engine/main.go
    docker-compose.yml
  </files>
  <action>
    **Update `cmd/reaction-engine/main.go`:**
    - Import: `internal/observability`, `internal/dlq`
    - After logger setup: create observability module `obs, err := observability.New("reaction-engine")`
    - Defer `obs.Shutdown(ctx)`
    - Create metrics: `metrics, err := observability.NewMetrics(obs.Meter())`
    - Start a small HTTP server on configurable port (default: `:9091`, env: `METRICS_ADDR`) serving `/metrics` (promhttp.Handler) and `/health` (200 OK). Same pattern as warehouse-sink.
    - After NATS connection: create and ensure DLQ stream
      ```go
      if err := streamMgr.EnsureDLQStream(ctx); err != nil {
          return fmt.Errorf("failed to ensure DLQ stream: %w", err)
      }
      ```
    - Create DLQ module: `dlqMod := dlq.New(natsClient.JetStream(), natsClient.Conn(), cfg.NATS.Stream.Name, []string{cfg.ConsumerName}, dlqCfg, metrics, logger)`
    - Start DLQ: `dlqMod.Start(ctx)`
    - Pass metrics to consumer via NewConsumer
    - On shutdown: stop DLQ, then consumer, then engine, anomaly, dispatcher
    - Add shutdown timeout: use `context.WithTimeout(context.Background(), shutdownTimeout)` for consumer.Stop
    - Add `MetricsAddr` and `ShutdownTimeout` to Config struct

    **Update `docker-compose.yml`:**
    - Add to `reaction-engine` service:
      ```yaml
      METRICS_ADDR: ":9091"
      SHUTDOWN_TIMEOUT: "30s"
      CONSUMER_WORKER_COUNT: "1"
      ```
    - Expose port 9091 for metrics scraping
    - Add `stop_grace_period: 1m` (reaction engine doesn't need as long as warehouse sink)
    - Ensure `natsClient.Conn()` method exists (check `internal/nats/client.go` — if not, add a `Conn() *nats.Conn` accessor method to the Client struct)
  </action>
  <verify>
    - `go build ./cmd/reaction-engine/...` compiles
    - `grep "observability" cmd/reaction-engine/main.go` shows observability wired in
    - `grep "dlq" cmd/reaction-engine/main.go` shows DLQ wired in
    - `grep "METRICS_ADDR" docker-compose.yml` shows metrics config for reaction-engine
    - `grep "stop_grace_period" docker-compose.yml` shows value for reaction-engine
  </verify>
  <done>
    Reaction engine has observability (Prometheus metrics at /metrics), DLQ advisory listener, worker pool, and graceful shutdown with timeout. Docker Compose updated with appropriate config.
  </done>
</task>

</tasks>

<verification>
- `go build ./cmd/reaction-engine/...` compiles
- Reaction consumer has worker pool pattern
- Poison messages are terminated (not retried)
- Metrics endpoint serves Prometheus format
- DLQ module captures failed messages
- Graceful shutdown with timeout
- Docker Compose has metrics port and stop_grace_period
</verification>

<success_criteria>
- Reaction engine exports Prometheus metrics at /metrics
- Worker pool enables parallel message consumption
- Poison messages terminated via msg.Term()
- DLQ advisory listener captures max-delivery events
- Graceful shutdown with configurable timeout
- NATSMessagesProcessed metric incremented per message
- Docker Compose properly configured
</success_criteria>

<output>
After completion, create `.planning/phases/01-pipeline-hardening-observability-go-sdk/01-08-SUMMARY.md`
</output>
